# LLM-Evaluation-Portfolio-
Clean-room prompt engineering and LLM evaluation projects demonstrating instruction writing, rubric design, grounded generation, and reasoning evaluation using synthetic data.
#3 Overview

This repository contains clean-room prompt engineering and LLM evaluation projects designed to demonstrate instruction writing, rubric design, grounded generation, and reasoning evaluation using fully synthetic data.

The focus is on **reliability, constraint enforcement, and evaluatability**, rather than creative prompting or model-specific tricks.

---

## What This Repository Demonstrates

Across the included projects, this portfolio emphasizes:

- Instruction-following under strict constraints  
- Grounded generation with explicit handling of missing or conflicting data  
- Objective evaluation via rubrics and pass/fail criteria  
- Reasoning-focused test cases and failure-mode analysis  
- Iterative prom
